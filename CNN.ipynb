{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53739f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a225a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "True\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # fallback\n",
    "print(\"Using device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e95c990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fashion-mnist_train.csv')\n",
    "df.head()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a444471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split data\n",
    "x = df.iloc[:,1:].values #pixel values starting from col 1\n",
    "y = df.iloc[:,0].values # y is the label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1be27720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_full shape: (48000, 784)\n",
      "y_train_full shape: (48000,)\n",
      "x_test shape: (12000, 784)\n",
      "y_test shape: (12000,)\n"
     ]
    }
   ],
   "source": [
    "x_train_full , x_test, y_train_full, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "print(\"x_train_full shape:\", x_train_full.shape)\n",
    "print(\"y_train_full shape:\", y_train_full.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da320f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_full, y_train_full, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "#scaling the features ; vals 0 to 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_val = x_val / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42b8cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# data augmentation // transforming the training dataset \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(0, translate=(0.1,0.1)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f16c5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create customdataset class\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels,transform = None):\n",
    "        #convert to pytroch tensors\n",
    "        self.features = torch.tensor(features, dtype=torch.float32).reshape(-1,1,28,28) ## change -1 \n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature, label = self.features[index], self.labels[index]\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature.squeeze(0).numpy()) #apply transformation\n",
    "        return feature, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01a99d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataset object\n",
    "train_dataset = CustomDataset(x_train,y_train, transform=train_transform)\n",
    "#create test dataset object\n",
    "test_dataset = CustomDataset(x_test,y_test, transform=test_transform)\n",
    "val_dataset = CustomDataset(x_val,y_val,transform=test_transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dd5e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNN(nn.Module):\n",
    "    def __init__(self,num_conv_layers, num_filters, kernel_size, num_fc_layers,fc_layer_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        layers=[]\n",
    "        in_channels = 1 #grayscale img\n",
    "\n",
    "        #conv layers\n",
    "        for _ in range(num_conv_layers):\n",
    "            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=kernel_size,padding='same'))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm2d(num_filters))\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_channels=num_filters #for next layer\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        #FCC / ANN:\n",
    "        fc_layers = [nn.Flatten()]\n",
    "        input_size = num_filters * (28//(2**num_conv_layers)) **2\n",
    "        for _ in range(num_fc_layers):\n",
    "            fc_layers.append(nn.Linear(input_size, fc_layer_size))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "            fc_layers.append(nn.Dropout(dropout_rate))\n",
    "            input_size=fc_layer_size\n",
    "        fc_layers.append(nn.Linear(input_size,10)) #output layer , 10 classes\n",
    "\n",
    "        self.classifier = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ac249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna obj func:\n",
    "\n",
    "def objective(trial):\n",
    "    num_conv_layers = trial.suggest_int('num_conv_layers', 1, 3)\n",
    "    num_filters = trial.suggest_categorical('num_filters', [16, 32, 64, 128])\n",
    "    kernel_size = trial.suggest_categorical('kernel_size', [3, 5])\n",
    "    num_fc_layers = trial.suggest_int('num_fc_layers', 1, 3)\n",
    "    fc_layer_size = trial.suggest_categorical('fc_layer_size', [64, 128, 256])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['SGD', 'Adam', 'RMSprop'])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 30)\n",
    "\n",
    "    model = myNN(num_conv_layers, num_filters, kernel_size, num_fc_layers,fc_layer_size, dropout_rate).to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "    #optimizer = optim.SGD(model.parameters(), lr = 0.1 , weight_decay=1e-4)\n",
    "\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "       optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer =optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    #loss func\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs,batch_labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "    #validation loop // eval\n",
    "    model.eval()\n",
    " \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in val_loader:\n",
    "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += batch_labels.size(0)\n",
    "            val_correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "\n",
    "\n",
    "   # return test_acc\n",
    "    return val_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b071af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-05 16:23:32,268] A new study created in memory with name: no-name-d201cfe3-ca84-47e3-a1fe-abeefd3e8d51\n",
      "/var/folders/ql/2hfqj6yn70953z06w6kfhjm40000gn/T/ipykernel_87110/3392404378.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/var/folders/ql/2hfqj6yn70953z06w6kfhjm40000gn/T/ipykernel_87110/3392404378.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
      "/var/folders/ql/2hfqj6yn70953z06w6kfhjm40000gn/T/ipykernel_87110/3392404378.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/Users/mustafasaqib/Desktop/Code/AnimalClassify/animal-classifier-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-05 16:25:03,456] Trial 0 finished with value: 0.8639583333333334 and parameters: {'num_conv_layers': 2, 'num_filters': 16, 'kernel_size': 5, 'num_fc_layers': 2, 'fc_layer_size': 64, 'dropout_rate': 0.3577402833039748, 'weight_decay': 0.0010636238928688735, 'learning_rate': 0.0020469845104683533, 'optimizer': 'RMSprop', 'batch_size': 128, 'num_epochs': 14}. Best is trial 0 with value: 0.8639583333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8448\n",
      "Validation Accuracy: 0.8894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-05 16:30:14,013] Trial 1 finished with value: 0.889375 and parameters: {'num_conv_layers': 2, 'num_filters': 128, 'kernel_size': 3, 'num_fc_layers': 3, 'fc_layer_size': 128, 'dropout_rate': 0.49309874513333896, 'weight_decay': 0.0026066014121061028, 'learning_rate': 0.005087549409132183, 'optimizer': 'SGD', 'batch_size': 32, 'num_epochs': 16}. Best is trial 1 with value: 0.889375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8742\n",
      "Validation Accuracy: 0.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-05 16:38:42,919] Trial 2 finished with value: 0.9072916666666667 and parameters: {'num_conv_layers': 3, 'num_filters': 64, 'kernel_size': 5, 'num_fc_layers': 2, 'fc_layer_size': 256, 'dropout_rate': 0.43200349107299785, 'weight_decay': 5.158005301677908e-05, 'learning_rate': 0.0013249628145140544, 'optimizer': 'SGD', 'batch_size': 32, 'num_epochs': 30}. Best is trial 2 with value: 0.9072916666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9036\n"
     ]
    }
   ],
   "source": [
    "#run optuna\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction='maximize', pruner = pruner)\n",
    "study.optimize(objective, n_trials=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d3eb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_conv_layers': 3,\n",
       " 'num_filters': 64,\n",
       " 'kernel_size': 5,\n",
       " 'num_fc_layers': 2,\n",
       " 'fc_layer_size': 256,\n",
       " 'dropout_rate': 0.43200349107299785,\n",
       " 'weight_decay': 5.158005301677908e-05,\n",
       " 'learning_rate': 0.0013249628145140544,\n",
       " 'optimizer': 'SGD',\n",
       " 'batch_size': 32,\n",
       " 'num_epochs': 30}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abf91942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9072916666666667"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2075501f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 1.3444\n",
      "Epoch 1/30, Loss: 1.3444, Train Acc: 0.7475, Test Acc: 0.7678\n",
      "Epoch 2/30, Loss: 0.7396\n",
      "Epoch 2/30, Loss: 0.7396, Train Acc: 0.7964, Test Acc: 0.8088\n",
      "Epoch 3/30, Loss: 0.6099\n",
      "Epoch 3/30, Loss: 0.6099, Train Acc: 0.8226, Test Acc: 0.8355\n",
      "Epoch 4/30, Loss: 0.5391\n",
      "Epoch 4/30, Loss: 0.5391, Train Acc: 0.8411, Test Acc: 0.8523\n",
      "Epoch 5/30, Loss: 0.4975\n",
      "Epoch 5/30, Loss: 0.4975, Train Acc: 0.8495, Test Acc: 0.8573\n",
      "Epoch 6/30, Loss: 0.4657\n",
      "Epoch 6/30, Loss: 0.4657, Train Acc: 0.8598, Test Acc: 0.8719\n",
      "Epoch 7/30, Loss: 0.4382\n",
      "Epoch 7/30, Loss: 0.4382, Train Acc: 0.8666, Test Acc: 0.8759\n",
      "Epoch 8/30, Loss: 0.4196\n",
      "Epoch 8/30, Loss: 0.4196, Train Acc: 0.8708, Test Acc: 0.8807\n",
      "Epoch 9/30, Loss: 0.4058\n",
      "Epoch 9/30, Loss: 0.4058, Train Acc: 0.8705, Test Acc: 0.8821\n",
      "Epoch 10/30, Loss: 0.3861\n",
      "Epoch 10/30, Loss: 0.3861, Train Acc: 0.8822, Test Acc: 0.8901\n",
      "Epoch 11/30, Loss: 0.3790\n",
      "Epoch 11/30, Loss: 0.3790, Train Acc: 0.8838, Test Acc: 0.8901\n",
      "Epoch 12/30, Loss: 0.3689\n",
      "Epoch 12/30, Loss: 0.3689, Train Acc: 0.8809, Test Acc: 0.8917\n",
      "Epoch 13/30, Loss: 0.3579\n",
      "Epoch 13/30, Loss: 0.3579, Train Acc: 0.8903, Test Acc: 0.9006\n",
      "Epoch 14/30, Loss: 0.3488\n",
      "Epoch 14/30, Loss: 0.3488, Train Acc: 0.8921, Test Acc: 0.9018\n",
      "Epoch 15/30, Loss: 0.3422\n",
      "Epoch 15/30, Loss: 0.3422, Train Acc: 0.8881, Test Acc: 0.8984\n",
      "Epoch 16/30, Loss: 0.3346\n",
      "Epoch 16/30, Loss: 0.3346, Train Acc: 0.8985, Test Acc: 0.9047\n",
      "Epoch 17/30, Loss: 0.3298\n",
      "Epoch 17/30, Loss: 0.3298, Train Acc: 0.8913, Test Acc: 0.9019\n",
      "Epoch 18/30, Loss: 0.3264\n",
      "Epoch 18/30, Loss: 0.3264, Train Acc: 0.8989, Test Acc: 0.9057\n",
      "Epoch 19/30, Loss: 0.3198\n",
      "Epoch 19/30, Loss: 0.3198, Train Acc: 0.8943, Test Acc: 0.9009\n",
      "Epoch 20/30, Loss: 0.3156\n",
      "Epoch 20/30, Loss: 0.3156, Train Acc: 0.9035, Test Acc: 0.9091\n",
      "Epoch 21/30, Loss: 0.3108\n",
      "Epoch 21/30, Loss: 0.3108, Train Acc: 0.8976, Test Acc: 0.9062\n",
      "Epoch 22/30, Loss: 0.3104\n",
      "Epoch 22/30, Loss: 0.3104, Train Acc: 0.9066, Test Acc: 0.9082\n",
      "Epoch 23/30, Loss: 0.3028\n",
      "Epoch 23/30, Loss: 0.3028, Train Acc: 0.9079, Test Acc: 0.9096\n",
      "Epoch 24/30, Loss: 0.2985\n",
      "Epoch 24/30, Loss: 0.2985, Train Acc: 0.8842, Test Acc: 0.8963\n",
      "Epoch 25/30, Loss: 0.2947\n",
      "Epoch 25/30, Loss: 0.2947, Train Acc: 0.9084, Test Acc: 0.9124\n",
      "Epoch 26/30, Loss: 0.2902\n",
      "Epoch 26/30, Loss: 0.2902, Train Acc: 0.8915, Test Acc: 0.8979\n",
      "Epoch 27/30, Loss: 0.2885\n",
      "Epoch 27/30, Loss: 0.2885, Train Acc: 0.9025, Test Acc: 0.9081\n",
      "Epoch 28/30, Loss: 0.2829\n",
      "Epoch 28/30, Loss: 0.2829, Train Acc: 0.8969, Test Acc: 0.9042\n",
      "Epoch 29/30, Loss: 0.2846\n",
      "Epoch 29/30, Loss: 0.2846, Train Acc: 0.9110, Test Acc: 0.9158\n",
      "Epoch 30/30, Loss: 0.2805\n",
      "Epoch 30/30, Loss: 0.2805, Train Acc: 0.9117, Test Acc: 0.9134\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "\n",
    "# merge train+val for final training\n",
    "final_train_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "final_train_loader = DataLoader(final_train_dataset,\n",
    "                                batch_size=best_params[\"batch_size\"],\n",
    "                                shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, pin_memory=True)\n",
    "\n",
    "model = myNN(num_conv_layers=best_params['num_conv_layers'],\n",
    "    num_filters=best_params['num_filters'],\n",
    "    kernel_size=best_params['kernel_size'],\n",
    "    num_fc_layers=best_params['num_fc_layers'],\n",
    "    fc_layer_size=best_params['fc_layer_size'],\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "if best_params['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "elif best_params['optimizer'] == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "else:\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "\n",
    "#loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#training loop\n",
    "epochs = best_params['num_epochs']\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_epoch_loss = 0\n",
    "\n",
    "    for batch_features, batch_labels in final_train_loader:\n",
    "        batch_features, batch_labels = batch_features.to(device) , batch_labels.to(device)\n",
    "\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_epoch_loss / len(final_train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "#TRAIN ACCURACY\n",
    "    train_correct, train_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in final_train_loader:\n",
    "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += batch_labels.size(0)\n",
    "            train_correct += (predicted == batch_labels).sum().item()\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    #TEST ACCURACY\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += batch_labels.size(0)\n",
    "            test_correct += (predicted == batch_labels).sum().item()\n",
    "    test_acc = test_correct / test_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    #train mode for next epoch\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results:\n",
    "# Epoch 28/30, Loss: 0.2829, Train Acc: 0.8969, Test Acc: 0.9042\n",
    "# Epoch 29/30, Loss: 0.2846\n",
    "# Epoch 29/30, Loss: 0.2846, Train Acc: 0.9110, Test Acc: 0.9158\n",
    "# Epoch 30/30, Loss: 0.2805\n",
    "# Epoch 30/30, Loss: 0.2805, Train Acc: 0.9117, Test Acc: 0.9134"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animal-classifier-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
